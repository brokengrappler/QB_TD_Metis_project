{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time, os\n",
    "import random\n",
    "from fake_useragent import UserAgent\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from IPython.core.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Look at 2019, look at 2002\\nadd from both\\nlook at median year, add\\nthen start checking between 2019 and median year?'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' Look at 2019, look at 2002\n",
    "add from both\n",
    "look at median year, add\n",
    "then start checking between 2019 and median year?\n",
    "\n",
    "But for actually grabbing the data... this is going to be a bit confusing\n",
    "Maybe I need to just click back for 10 years and scrap this craping idea below.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average QB career is 3.3 years. Perhaps capture QBs who have been active in the last 5 years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qb_soup_scraper():\n",
    "    '''\n",
    "    Create a list of soup files to search through. Adjust years_list variable to adjust years to scrape.\n",
    "    return:\n",
    "        list of soup files that contain QB names\n",
    "    '''\n",
    "    base_url = 'https://www.pro-football-reference.com/years/'\n",
    "    html = '/passing.htm'\n",
    "    years_list = list(map(str,[x for x in range(2010,2019)]))\n",
    "    ua = UserAgent()\n",
    "    user_agent = {'User-agent' : ua.random}\n",
    "    \n",
    "    soup_list=[]\n",
    "    \n",
    "    for year in years_list:\n",
    "        response = requests.get(base_url + year + html, headers = user_agent)\n",
    "        # I would insert a try and raise test here to check response but nervouse about scraping page. Try later.\n",
    "        soup = BeautifulSoup(response.text, 'lxml')\n",
    "        soup_list.append(soup)\n",
    "        time.sleep(.5+2*random.random())\n",
    "    return soup_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup_list = qb_soup_scraper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "def raw_qb_list(soup):\n",
    "    qb_list = []\n",
    "    links = soup.find_all('tbody')\n",
    "    for qbpos in links:\n",
    "        qb_names = qbpos.find_all('a')\n",
    "        for names in qb_names:\n",
    "            str_test = names.text.split()\n",
    "            if len(str_test) > 1:\n",
    "                qb_list.append(str_test)\n",
    "    qb_list = [' '.join(name) for name in qb_list]\n",
    "    return qb_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def raw_pos_list(soup):\n",
    "    position_list=[]\n",
    "    pos_soup = soup.find_all('td', {'data-stat':'pos'})\n",
    "    for x in pos_soup:\n",
    "        position_list.append(x.text.lower())\n",
    "    return position_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "def raw_scrape(soup_list):\n",
    "    qb_filter_dict = {}\n",
    "    qb_master_list = []\n",
    "    qb_temp_list = []\n",
    "    pos_list = []\n",
    "    \n",
    "    for soup in soup_list:\n",
    "        qb_temp_list = raw_qb_list(soup)\n",
    "        pos_list = raw_pos_list(soup)\n",
    "        # Create df to filter either obscure or non-QBs\n",
    "        filter_df = pd.DataFrame()\n",
    "        filter_df['name'] = qb_temp_list\n",
    "        filter_df['pos'] = pos_list\n",
    "        filter_df = filter_df[filter_df['pos'] == 'qb']\n",
    "        qb_temp_list = list(filter_df['name'])\n",
    "        for qb in qb_temp_list:\n",
    "            if qb not in qb_master_list:\n",
    "                qb_master_list.append(qb)\n",
    "    \n",
    "    return qb_master_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102\n",
      "102\n",
      "96\n",
      "96\n",
      "85\n",
      "85\n",
      "85\n",
      "85\n",
      "100\n",
      "100\n",
      "86\n",
      "86\n",
      "96\n",
      "96\n",
      "94\n",
      "94\n",
      "106\n",
      "106\n"
     ]
    }
   ],
   "source": [
    "qb_master_list = raw_scrape(soup_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:metis] *",
   "language": "python",
   "name": "conda-env-metis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
